Warning: 100 points required, but 125 points sampled.
Warning: 10000 points required, but 12500 points sampled.
Compiling model...
'compile' took 2.777224 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [2.72e-01, 6.40e+02, 7.43e-02]    [3.02e-01, 6.40e+02, 7.43e-02]    []

Best model at step 0:
  train loss: 6.40e+02
  test loss: 6.41e+02
  test metric: []

'train' took 2.813834 s

Compiling model...
'compile' took 0.001476 s

The optimizer_name is: adam
Training model...

Step      Train loss                        Test loss                         Test metric
0         [3.00e+00, 3.00e+00, 3.00e+00]    [3.33e+00, 3.00e+00, 3.00e+00]    []
Traceback (most recent call last):
  File "/working_dir/./src/main.py", line 50, in <module>
    main()
  File "/opt/conda/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/opt/conda/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/working_dir/./src/main.py", line 47, in main
    train.single_observer(prj, run, "0", cfg.network)
  File "/working_dir/src/train.py", line 111, in single_observer
    return mo, metrics
  File "/working_dir/src/train.py", line 87, in train_model
    def single_observer(name_prj, name_run, n_test, cfg):
  File "/working_dir/src/train.py", line 134, in train_and_save_model
    )
  File "/opt/conda/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 650, in train
    self._train_sgd(iterations, display_every)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 668, in _train_sgd
    self._train_step(
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 562, in _train_step
    self.train_step(inputs, targets, auxiliary_vars)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 362, in train_step
    self.opt.step(closure)
  File "/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py", line 146, in step
    loss = closure()
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 359, in closure
    total_loss.backward()
  File "/opt/conda/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
