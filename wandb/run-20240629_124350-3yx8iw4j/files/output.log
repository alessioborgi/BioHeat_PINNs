Warning: 100 points required, but 125 points sampled.
Warning: 10000 points required, but 12500 points sampled.
Compiling model...
'compile' took 2.948004 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [2.72e-01, 6.40e+02, 7.43e-02]    [3.02e-01, 6.40e+02, 7.43e-02]    []

Best model at step 0:
  train loss: 6.40e+02
  test loss: 6.41e+02
  test metric: []

'train' took 3.280095 s

Compiling model...
'compile' took 0.001873 s

Training model...

Step      Train loss                        Test loss                         Test metric
0         [3.00e+00, 3.00e+00, 3.00e+00]    [3.33e+00, 3.00e+00, 3.00e+00]    []
100       [1.63e-01, 3.87e-02, 1.49e-01]    [1.37e-01, 3.87e-02, 1.49e-01]    []
Traceback (most recent call last):
  File "/working_dir/./src/main.py", line 50, in <module>
    main()
  File "/opt/conda/lib/python3.10/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/opt/conda/lib/python3.10/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/opt/conda/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/working_dir/./src/main.py", line 47, in main
    train.single_observer(prj, run, "0", cfg.network)
  File "/working_dir/src/train.py", line 111, in single_observer
    mo = train_model(name_run, cfg)
  File "/working_dir/src/train.py", line 87, in train_model
    losshistory, train_state = train_and_save_model(mm, epochs, callbacks, "adam")
  File "/working_dir/src/train.py", line 133, in train_and_save_model
    losshistory, train_state = model.train(
  File "/opt/conda/lib/python3.10/site-packages/deepxde/utils/internal.py", line 22, in wrapper
    result = f(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 650, in train
    self._train_sgd(iterations, display_every)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 668, in _train_sgd
    self._train_step(
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 562, in _train_step
    self.train_step(inputs, targets, auxiliary_vars)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 362, in train_step
    self.opt.step(closure)
  File "/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py", line 146, in step
    loss = closure()
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 356, in closure
    losses = outputs_losses_train(inputs, targets, auxiliary_vars)[1]
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 320, in outputs_losses_train
    return outputs_losses(
  File "/opt/conda/lib/python3.10/site-packages/deepxde/model.py", line 308, in outputs_losses
    losses = losses_fn(targets, outputs_, loss_fn, inputs, self, aux=aux)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/data/data.py", line 13, in losses_train
    return self.losses(targets, outputs, loss_fn, inputs, model, aux=aux)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/data/pde.py", line 146, in losses
    f = self.pde(inputs, outputs_pde)
  File "/working_dir/src/pde.py", line 103, in pde
    dy_xx = dde.grad.hessian(y, x, i=0, j=0)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/gradients/gradients.py", line 61, in hessian
    return gradients_reverse.hessian(ys, xs, component=component, i=i, j=j)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/gradients/gradients_reverse.py", line 140, in hessian
    return hessian._Hessians(ys, xs, component=component, i=i, j=j)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/gradients/gradients_reverse.py", line 132, in __call__
    return self.Hs[key](i, j)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/gradients/gradients_reverse.py", line 109, in __call__
    return self.H(j, i)
  File "/opt/conda/lib/python3.10/site-packages/deepxde/gradients/gradients_reverse.py", line 29, in __call__
    self.J[i] = torch.autograd.grad(
  File "/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py", line 411, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
