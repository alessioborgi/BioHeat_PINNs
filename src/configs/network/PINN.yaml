# PINN Configurations Static
backend: pytorch
LBFGS: false
learning_rate: 0.001
num_dense_layers: 4
num_dense_nodes: 50
output_injection_gain: 50
resampling: true
resampler_period: 50
initial_weights_regularizer: true



# PINN Configurations subject to fine-tuning.
## ELU, GELU, ReLU, SELU, Sigmoid, SiLU, sin, Swish, tanh, Mish, Softplus, APTx
## Glorot normal, He normal, Orthogonal, Random normal, Random uniform, Zeros

activation: SiLU     
iterations: 1000
initialization: He normal



# TUNING CONFIGURATIONS
# activation: "???"  # Placeholder for tuning the activation function.
# iterations: "???"  # Placeholder for tuning the number of iterations (epochs)
# initialization: "???"  # Placeholder for tuning the initialization method.