# PINN Configurations
backend: pytorch
#ELU, GELU, ReLU, SELU, Sigmoid, SiLU, sin, Swish, tanh, Mish, Softplus, APTx
activation: ELU     
initial_weights_regularizer: true
# Glorot normal, He normal, LeCun normal, Orthogonal, Random normal, Random uniform, Zeros
initialization: He normal
iterations: 3000
LBFGS: false
learning_rate: 0.001
num_dense_layers: 2
num_dense_nodes: 50
output_injection_gain: 50
resampling: true
resampler_period: 100